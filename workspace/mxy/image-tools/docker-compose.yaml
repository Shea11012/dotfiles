name: image-tools

networks:
  tools:
    driver: bridge

services:
  network-service:
    image: alpine
    container_name: tool-network
    ports:
      - "3003:3001" # gpt-load
      - '3004:8080' # octopus 
    command: tail -f /dev/null
    networks:
      - tools

  caddy:
    build: ./caddy
    container_name: caddy-upload
    user: "1000:1000"
    cap_add:
      - NET_ADMIN
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"
    volumes:
      - ./caddy/conf:/etc/caddy
      - ./caddy/assets:/srv
    restart: unless-stopped

  openlist:
    image: openlistteam/openlist:latest-aio
    container_name: openlist
    user: "1000:1000"
    volumes:
      - "./openlist/data:/opt/openlist/data"
    ports:
      - "5244:5244"
    environment:
      - UMASK=022
      - TZ=Asia/Shanghai
      - OPENLIST_ADMIN_PASSWORD=admin
    restart: unless-stopped

  gpt-load:
    image: ghcr.io/tbphp/gpt-load:latest
    container_name: gpt-load
    restart: always
    user: "1000:1000"
    volumes:
      - ./gpt-load/data:/app/data
    env_file:
      - ./gpt-load/.env
    healthcheck:
      test: wget -q --spider -T 10 -O /dev/null http://localhost:${PORT:-3001}/health
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    network_mode: "service:network-service"

  watchtower:
    image: containrrr/watchtower
    container_name: watchtower
    environment:
      - TZ=Asia/Shanghai
    volumes:
      # - /run/user/1000/docker.sock:/var/run/docker.sock
      - /var/run/docker.sock:/var/run/docker.sock
    command: watchtower gpt-load openlist --cleanup --schedule "0 0 1 * * *"
    restart: on-failure:3
    network_mode: "service:network-service"
    # networks:
    #   - tools
